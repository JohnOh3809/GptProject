{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b8852a",
   "metadata": {},
   "source": [
    "# Dialogue GPT: Final Project\n",
    "\n",
    "## Overview\n",
    "\n",
    "Put everything together to train your own language model from scratch. You'll assemble all components from Days 1-4, implement KV caching for efficient generation, train on a text corpus, and analyze the results.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Model Architecture\n",
    "\n",
    "### Required Components\n",
    "\n",
    "Assemble your model using implementations from previous days:\n",
    "\n",
    "**From Day 1:**\n",
    "- Multi-head self-attention\n",
    "- Causal masking for autoregressive generation\n",
    "\n",
    "**From Day 2:**\n",
    "- Implement 3 variants with different positional encodings:\n",
    "  - Sinusoidal positional encoding\n",
    "  - RoPE (Rotary Position Embedding)\n",
    "  - ALiBi (Attention with Linear Biases)\n",
    "\n",
    "**From Day 3:**\n",
    "- Character-level or BPE tokenizer (your choice)\n",
    "\n",
    "**From Day 4:**\n",
    "- Layer normalization\n",
    "- Transformer blocks (attention + FFN + residual)\n",
    "- Full decoder LM with embedding layer and output head\n",
    "\n",
    "**Additional Modules:**\n",
    "- **Training scripts**\n",
    "- **KV caching** for efficient generation (see below)\n",
    "\n",
    "### Model Specifications\n",
    "\n",
    "Choose reasonable hyperparameters:\n",
    "- `d_model`: 128-256\n",
    "- `num_heads`: 4-8\n",
    "- `num_layers`: 4-8\n",
    "- `d_ff`: 4 * d_model\n",
    "- `max_seq_len`: 128-512\n",
    "- `vocab_size`: depends on tokenizer\n",
    "\n",
    "### KV Caching Implementation\n",
    "\n",
    "During autoregressive generation, computing attention for all previous tokens at each step is wasteful. **KV caching** stores previously computed Key and Value matrices.\n",
    "\n",
    "**How it works:**\n",
    "1. On first forward pass, compute K and V for all input tokens\n",
    "2. For each new token, only compute K and V for that token\n",
    "3. Concatenate new K, V with cached values\n",
    "4. Compute attention using cached KV pairs\n",
    "\n",
    "**Implementation requirements:**\n",
    "- Modify your attention module to accept and return optional `past_kv` cache\n",
    "- Cache format: list of tuples `[(K_layer1, V_layer1), (K_layer2, V_layer2), ...]`\n",
    "- Support both training mode (no cache) and generation mode (with cache)\n",
    "- Ensure cache works correctly with all three positional encodings\n",
    "\n",
    "**Expected speedup:** 3-10x faster generation for sequences > 100 tokens\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Training Setup\n",
    "\n",
    "### Dataset Options\n",
    "\n",
    "Choose a corpus to train on. Some examples are:\n",
    "\n",
    "1. **Complete Works of Shakespeare** (~5MB)\n",
    "   - Download: `wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt`\n",
    "   \n",
    "2. **OpenWebText subset** (download ~100MB sample)\n",
    "   - Download: `https://huggingface.co/datasets/Skylion007/openwebtext`\n",
    "   \n",
    "### Training Details\n",
    "\n",
    "**Setup:**\n",
    "- Train/validation split: 90/10\n",
    "- Batch size: 32-64 (depends on GPU memory)\n",
    "- Sequence length: Match your `max_seq_len`\n",
    "\n",
    "**Optimizer:**\n",
    "- AdamW with weight decay (0.01)\n",
    "- Implement learning rate schedule: warmup + cosine decay (see below)\n",
    "  - Max LR: 3e-4 to 1e-3\n",
    "  - Min LR: 1e-5\n",
    "  - Warmup: 5-10% of total steps\n",
    "\n",
    "**Training:**\n",
    "- Train for at least 10,000 steps (more is better)\n",
    "- Evaluate every 500-1000 steps\n",
    "- Track: training loss, validation loss, perplexity\n",
    "- Save best checkpoint based on validation loss\n",
    "\n",
    "**Training Loop Pseudocode:**\n",
    "```python\n",
    "# Setup\n",
    "optimizer = AdamW(model.parameters(), weight_decay=0.01)\n",
    "criterion = CrossEntropyLoss()\n",
    "best_val_loss = infinity\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        # 1. Update learning rate based on schedule\n",
    "        lr = get_lr(global_step, warmup_steps, total_steps, max_lr, min_lr)\n",
    "        set_optimizer_lr(optimizer, lr)\n",
    "        \n",
    "        # 2. Forward pass\n",
    "        logits = model(batch_inputs)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        # 3. Compute loss (reshape for cross-entropy)\n",
    "        loss = criterion(\n",
    "            logits.view(-1, vocab_size),  # (batch*seq_len, vocab_size)\n",
    "            batch_targets.view(-1)         # (batch*seq_len,)\n",
    "        )\n",
    "        \n",
    "        # 4. Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    # 5. Periodic validation\n",
    "    if (epoch + 1) % eval_interval == 0:\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "        \n",
    "        # 6. Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model)\n",
    "\n",
    "# 7. Restore best model for inference\n",
    "load_checkpoint(model, best_checkpoint)\n",
    "```\n",
    "\n",
    "### Learning Rate Schedule Implementation\n",
    "\n",
    "Modern transformers use **warmup followed by cosine decay** instead of a fixed learning rate.\n",
    "\n",
    "**Why warmup?**\n",
    "- Model weights start randomly initialized with large, unstable gradients\n",
    "- Starting with a small LR and gradually increasing prevents the model from diverging early\n",
    "- Allows optimizer momentum statistics (in Adam) to stabilize\n",
    "\n",
    "**Warmup phase** (first `warmup_steps` steps):\n",
    "```python\n",
    "lr = max_lr * (step / warmup_steps)  # Linear increase from 0 to max_lr\n",
    "```\n",
    "\n",
    "**Cosine decay phase** (remaining steps):\n",
    "```python\n",
    "progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
    "```\n",
    "\n",
    "**Implementation:**\n",
    "- Calculate `total_steps = num_epochs * steps_per_epoch`\n",
    "- At each training step, compute current LR and update optimizer: `optimizer.param_groups[0]['lr'] = lr`\n",
    "- Cosine decay smoothly reduces LR from `max_lr` to `min_lr`, helping the model converge better than linear decay\n",
    "\n",
    "**Expected behavior:** LR increases linearly during warmup, then follows a smooth cosine curve down to minimum\n",
    "\n",
    "### Perplexity Metric\n",
    "\n",
    "**Perplexity** is the standard metric for evaluating language models. It measures how \"surprised\" the model is by the test data.\n",
    "\n",
    "**Formula:**\n",
    "```python\n",
    "perplexity = math.exp(average_cross_entropy_loss)\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- Perplexity represents the **effective vocabulary size** the model is choosing from at each step\n",
    "- Perplexity of 10 means: on average, the model is as uncertain as if choosing uniformly from 10 tokens\n",
    "- **Lower is better** - perplexity of 1 would mean perfect prediction\n",
    "- Random guessing gives perplexity equal to vocabulary size\n",
    "\n",
    "**Example:** If your model has validation loss of 2.5, perplexity = e^2.5 ≈ 12.2\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "def compute_perplexity(model, dataloader):\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    for batch in dataloader:\n",
    "        loss = compute_loss(model, batch)\n",
    "        total_loss += loss.item() * batch.size(0)\n",
    "        total_tokens += batch.size(0)\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    return math.exp(avg_loss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Analysis & Experiments\n",
    "\n",
    "### 3.1 Training Curves\n",
    "\n",
    "Plot for each positional encoding variant:\n",
    "- Training loss vs. steps\n",
    "- Validation loss vs. steps  \n",
    "- Perplexity over time\n",
    "\n",
    "Also create one learning rate schedule plot showing:\n",
    "- Warmup phase (linear increase)\n",
    "- Cosine decay phase (smooth decrease)\n",
    "- Label the transition point between phases\n",
    "\n",
    "### 3.2 Positional Encoding Comparison\n",
    "\n",
    "Create a comparison table:\n",
    "\n",
    "| Metric | Sinusoidal | RoPE | ALiBi |\n",
    "|--------|-----------|------|-------|\n",
    "| Final train loss | | | |\n",
    "| Final val loss | | | |\n",
    "| Final perplexity | | | |\n",
    "| Training time | | | |\n",
    "| Best generation quality (subjective 1-5) | | | |\n",
    "\n",
    "**Analysis questions:**\n",
    "- Which positional encoding converged fastest?\n",
    "- Which achieved lowest validation loss?\n",
    "- Which generalizes best to longer sequences than training length?\n",
    "\n",
    "### 3.3 Attention Pattern Visualization\n",
    "\n",
    "For your best model, visualize attention patterns:\n",
    "\n",
    "1. **Select 3-5 interesting generated sequences**\n",
    "2. **Plot attention heatmaps** for different layers and heads:\n",
    "   - Early layers vs. late layers\n",
    "   - Different attention heads in the same layer\n",
    "3. **Analyze patterns:**\n",
    "   - Do you see local vs. global attention patterns?\n",
    "   - Do different heads specialize (e.g., previous token, syntactic structure)?\n",
    "   - How does attention change across layers?\n",
    "\n",
    "**Hint:** Extract attention weights from your model during forward pass. Plot using `plt.imshow()` or `sns.heatmap()`.\n",
    "\n",
    "### 3.4 KV Caching Performance\n",
    "\n",
    "Measure the speedup from KV caching:\n",
    "\n",
    "1. **Generate 500 tokens** with and without KV caching\n",
    "2. **Time both approaches** and calculate speedup ratio\n",
    "3. **Plot generation time vs. sequence length** (test at lengths: 50, 100, 200, 500)\n",
    "4. **Verify correctness:** Ensure outputs are identical with/without caching\n",
    "\n",
    "**Expected results:**\n",
    "- Speedup should increase with sequence length\n",
    "- Plot should show linear time (with cache) vs quadratic (without cache)\n",
    "\n",
    "### 3.5 Text Generation\n",
    "\n",
    "Generate text with different settings:\n",
    "\n",
    "**Prompts to try:**\n",
    "- Continuation of famous quotes from your corpus\n",
    "- Generic prompts like \"Once upon a time\"\n",
    "- Unusual prompts to test generalization\n",
    "\n",
    "**Sampling strategies:**\n",
    "- Temperature: {0.3, 0.7, 1.0, 1.5}\n",
    "- Top-p: {0.5, 0.9, 0.95}\n",
    "\n",
    "**Show 5-10 best examples** demonstrating:\n",
    "- Coherent long-form generation (200+ tokens)\n",
    "- Diverse outputs from same prompt\n",
    "- Model's understanding of style/domain\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Deliverable\n",
    "\n",
    "### Final Jupyter Notebook\n",
    "\n",
    "Your submission should be a **single well-organized notebook** containing:\n",
    "\n",
    "#### 1. Introduction\n",
    "- Brief description of your model architecture\n",
    "- Dataset choice and statistics\n",
    "- Hyperparameters used\n",
    "\n",
    "#### 2. Training Results\n",
    "- All training curves (losses, perplexity over time)\n",
    "- Learning rate schedule plot showing warmup and cosine decay\n",
    "- Comparison table for positional encodings\n",
    "- Discussion: What worked? What didn't?\n",
    "\n",
    "#### 3. KV Caching Analysis\n",
    "- Speedup measurements and plots\n",
    "- Correctness verification\n",
    "- Discussion of implementation challenges\n",
    "\n",
    "#### 4. Attention Analysis\n",
    "- 5-10 attention heatmaps from interesting examples\n",
    "- Written analysis of patterns observed\n",
    "- Comparison across layers and heads\n",
    "\n",
    "#### 5. Text Generation Showcase\n",
    "- 10-15 generation examples with different settings\n",
    "- Best examples demonstrating quality\n",
    "- Failure cases and analysis\n",
    "\n",
    "#### 6. Conclusions\n",
    "- What did you learn about LLMs?\n",
    "- Surprising findings?\n",
    "- What would you improve with more time/compute?\n",
    "- Impact of KV caching on production systems\n",
    "\n",
    "### Code Quality\n",
    "- Clean, well-commented code\n",
    "- Reproducible (set random seeds)\n",
    "- Model checkpoint saved\n",
    "- Include requirements.txt if using external libraries\n",
    "\n",
    "---\n",
    "\n",
    "## Grading Rubric (100 points)\n",
    "\n",
    "| Component | Points |\n",
    "|-----------|--------|\n",
    "| **Model Implementation** | 25 |\n",
    "| - Correct assembly of all components | 10 |\n",
    "| - Three positional encoding variants | 8 |\n",
    "| - KV caching implementation | 7 |\n",
    "| **Training** | 20 |\n",
    "| - Proper training loop with validation | 6 |\n",
    "| - Warmup + cosine LR schedule implemented | 6 |\n",
    "| - Training for sufficient steps | 4 |\n",
    "| - Clear training curves + perplexity tracking | 4 |\n",
    "| **Positional Encoding Comparison** | 15 |\n",
    "| - All three variants trained | 8 |\n",
    "| - Quantitative comparison | 4 |\n",
    "| - Thoughtful analysis | 3 |\n",
    "| **KV Caching Analysis** | 10 |\n",
    "| - Correct speedup measurements | 5 |\n",
    "| - Plots and correctness verification | 3 |\n",
    "| - Implementation discussion | 2 |\n",
    "| **Attention Visualization** | 12 |\n",
    "| - Multiple attention heatmaps | 6 |\n",
    "| - Insightful pattern analysis | 6 |\n",
    "| **Text Generation** | 13 |\n",
    "| - Diverse examples | 7 |\n",
    "| - Quality analysis | 6 |\n",
    "| **Presentation** | 5 |\n",
    "| - Organized notebook | 3 |\n",
    "| - Clear writing | 2 |\n",
    "\n",
    "---\n",
    "\n",
    "## Tips\n",
    "\n",
    "- **Start small:** Debug with tiny model on small dataset first\n",
    "- **Save checkpoints:** Training takes time, don't lose progress\n",
    "- **Use GPU:** Essential for reasonable training time (Google Colab free tier works)\n",
    "- **Monitor validation:** Stop if overfitting occurs\n",
    "- **Compare fairly:** Use same random seed for different positional encodings\n",
    "- **Attention extraction:** Modify your attention class to return weights as well as outputs\n",
    "- **KV caching tips:** Test with and without cache on short sequences first to verify correctness\n",
    "- **Cache dimensions:** Ensure cache tensors have correct shapes: `(batch_size, num_heads, seq_len, d_k)`\n",
    "- **LR schedule:** Plot your learning rate over training steps to verify warmup and decay work correctly\n",
    "- **Debugging training:** If loss explodes (NaN), reduce max_lr or increase warmup steps\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Tips\n",
    "\n",
    "If your model performs poorly, you can experiment with:\n",
    "1. **Try different architectures** (vary depth vs. width)\n",
    "2. **Compare tokenization strategies** (char vs BPE)\n",
    "3. **Fine-tune on a second domain** (transfer learning)\n",
    "4. **Implement beam search** instead of sampling\n",
    "5. **Multi-query attention** (MQA) or grouped-query attention (GQA) for faster inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6403f07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\" DO NOT CHANGE \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "\"\"\" DO NOT CHANGE \"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(2025)\n",
    "torch.manual_seed(2025)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b544a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "    def forward(self, query, key, value, mask=None, past_kv=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_q = query.size(1)\n",
    "        seq_len_k_new = key.size(1)\n",
    "        Q_proj = self.W_Q(query)\n",
    "        K_proj = self.W_K(key)\n",
    "        V_proj = self.W_V(value)\n",
    "        Q = Q_proj.view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K_proj.view(batch_size, seq_len_k_new, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V_proj.view(batch_size, seq_len_k_new, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        if past_kv is not None:\n",
    "            K_past, V_past = past_kv\n",
    "            K = torch.cat([K_past, K], dim=2)\n",
    "            V = torch.cat([V_past, V], dim=2)\n",
    "        present_kv = (K, V)\n",
    "        seq_len_k_full = K.size(2)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores + mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        output_attn = weights @ V\n",
    "        output_attn = output_attn.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        output = self.W_O(output_attn)\n",
    "\n",
    "        return output, weights, present_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09c3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPE(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: model dimension\n",
    "            max_len: maximum sequence length\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position=torch.arange(0, max_len).unsqueeze(1)\n",
    "        bottom=torch.exp(torch.arange(0, d_model,2)*(-np.log(10000)/d_model))\n",
    "        pe[:,0::2] = torch.sin(position*bottom)\n",
    "        pe[:,1::2] = torch.cos(position*bottom)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            x + positional_encoding: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        return x + self.pe[:, :x.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f029e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, d_model, base=10000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.base = base\n",
    "        frq = 1/(self.base**(torch.arange(0, d_model, 2)/d_model))\n",
    "        self.register_buffer('inv_freq',frq)\n",
    "\n",
    "    def _rotate_half(self, x):\n",
    "        rotated_x = torch.empty_like(x)\n",
    "        rotated_x[:,:, 0::2] = -x[:,:, 1::2]\n",
    "        rotated_x[:,:, 1::2] = x[:,:, 0::2]\n",
    "        return rotated_x\n",
    "\n",
    "    def apply_rope(self, q, k, positions):\n",
    "        freqs = positions.unsqueeze(-1)*self.inv_freq\n",
    "        freqs_expanded=torch.stack((freqs, freqs), dim=-1).flatten(-2, -1)\n",
    "        cos_vals=torch.cos(freqs_expanded)\n",
    "        sin_vals=torch.sin(freqs_expanded)\n",
    "        q_rot=q*cos_vals+self._rotate_half(q)*sin_vals\n",
    "        k_rot=k*cos_vals+ self._rotate_half(k)*sin_vals\n",
    "        return q_rot, k_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd7da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALiBi(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads=num_heads\n",
    "\n",
    "        slopes = torch.pow(2,-8 /num_heads*torch.arange(1, num_heads + 1, dtype=torch.float))\n",
    "        self.register_buffer('slopes', slopes.unsqueeze(1).unsqueeze(1))\n",
    "    def get_alibi_bias(self, seq_len):\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.float)\n",
    "        distance_matrix = torch.abs(positions.unsqueeze(0) - positions.unsqueeze(1))\n",
    "        alibi_bias = -self.slopes * distance_matrix.unsqueeze(0)\n",
    "        return alibi_bias\n",
    "    def forward(self, attention_scores, seq_len):\n",
    "        alibi_bias = self.get_alibi_bias(seq_len)\n",
    "        return attention_scores + alibi_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d6b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        \"\"\"\n",
    "        Layer normalization.\n",
    "\n",
    "        Args:\n",
    "            d_model: feature dimension\n",
    "            eps: small constant for numerical stability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma =nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply layer normalization.\n",
    "\n",
    "        Args:\n",
    "            x: (..., d_model) tensor\n",
    "\n",
    "        Returns:\n",
    "            normalized tensor of same shape\n",
    "        \"\"\"\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        var = torch.var(x, dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma*x_norm + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88baa3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=None):\n",
    "        \"\"\"\n",
    "        Decoder transformer block with pre-norm.\n",
    "\n",
    "        Args:\n",
    "            d_model: model dimension\n",
    "            num_heads: number of attention heads\n",
    "            d_ff: feed-forward hidden dimension (default: 4 * d_model)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "\n",
    "        self.ln1 = MyLayerNorm(d_model)\n",
    "        self.ln2 = MyLayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff,d_model)\n",
    "        )\n",
    "    def forward(self, x, past_kv=None):\n",
    "        \"\"\"\n",
    "        Forward pass with pre-norm residual connections.\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            past_kv: optional tuple of (K_cached, V_cached) for KV caching for this block.\n",
    "                     (batch, num_heads, prev_seq_len, d_k) per K/V.\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            present_kv: updated KV cache for this block.\n",
    "                        (K_present, V_present)\n",
    "        \"\"\"\n",
    "        attn_output, _, present_kv = self.attn(self.ln1(x), past_kv=past_kv)\n",
    "        x = x + attn_output\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x, present_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad914b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDecoderLM\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size, d_model, num_heads, num_layers, max_seq_len=\u001b[32m512\u001b[39m):\n\u001b[32m      3\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m        Decoder-only language model.\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[33;03m            max_seq_len: maximum sequence length (used for learned positional embeddings)\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class DecoderLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, max_seq_len=512):\n",
    "        \"\"\"\n",
    "        Decoder-only language model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            d_model: model dimension\n",
    "            num_heads: attention heads per layer\n",
    "            num_layers: number of transformer blocks\n",
    "            max_seq_len: maximum sequence length (used for learned positional embeddings)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, num_heads) for _ in range(num_layers)])\n",
    "\n",
    "        self.ln_f = MyLayerNorm(d_model)\n",
    "\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, x, past_kv_list=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: (batch, seq_len) token IDs. During generation with caching, seq_len is typically 1.\n",
    "            past_kv_list: optional list of tuples `[(K_layer1, V_layer1), ...]` for KV caching.\n",
    "                          Each (K, V) is (batch, num_heads, prev_seq_len, d_k).\n",
    "\n",
    "        Returns:\n",
    "            logits: (batch, seq_len, vocab_size)\n",
    "            present_kv_list: list of updated KV caches for all layers.\n",
    "                             `[(K_present_layer1, V_present_layer1), ...]`\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        tok_emb = self.token_embed(x)\n",
    "\n",
    "        if past_kv_list is not None:\n",
    "            past_seq_len = past_kv_list[0][0].shape[2] if past_kv_list else 0\n",
    "            current_position_idx = past_seq_len\n",
    "            positions_to_embed = torch.tensor([current_position_idx], device=x.device) # (1,)\n",
    "            pos_emb = self.pos_embed(positions_to_embed) # (1, d_model)\n",
    "            # Expand to (1, 1, d_model) for broadcasting with tok_emb (batch_size, 1, d_model)\n",
    "            pos_emb = pos_emb.unsqueeze(0)\n",
    "        else:\n",
    "            # Full sequence processing (training or first pass of generation)\n",
    "            positions = torch.arange(seq_len, device=x.device) # (seq_len,)\n",
    "            pos_emb = self.pos_embed(positions) # (seq_len, d_model)\n",
    "            # Expand to (1, seq_len, d_model) for broadcasting with tok_emb (batch_size, seq_len, d_model)\n",
    "            pos_emb = pos_emb.unsqueeze(0)\n",
    "\n",
    "\n",
    "        h = tok_emb + pos_emb # (batch, seq_len, d_model)\n",
    "\n",
    "        present_kv_list = []\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            past_kv_i = past_kv_list[i] if past_kv_list else None\n",
    "            h, present_kv_i = block(h, past_kv=past_kv_i)\n",
    "            present_kv_list.append(present_kv_i)\n",
    "\n",
    "        h = self.ln_f(h) # (batch, seq_len, d_model)\n",
    "        logits = self.head(h) # (batch, seq_len, vocab_size)\n",
    "\n",
    "        return logits, present_kv_list # Return updated KV cache list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c7521a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal self-attention layer that wraps MultiHeadAttention and applies a causal mask.\n",
    "    Handles passing through KV cache.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    def forward(self, x, past_kv=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            past_kv: Optional tuple of (K_cached, V_cached) for KV caching.\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            attn_weights: (batch, num_heads, seq_len, seq_len_k_full)\n",
    "            present_kv: Updated KV cache (K_present, V_present)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        if past_kv is not None:\n",
    "            mask = None\n",
    "        else:\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "            mask = mask.masked_fill(mask, float('-inf'))\n",
    "        output, attn_weights, present_kv = self.mha(x, x, x, mask=mask, past_kv=past_kv)\n",
    "        return output, attn_weights, present_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536780f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-28 22:37:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘data/input.txt’\n",
      "\n",
      "data/input.txt      100%[===================>]   1.06M  --.-KB/s    in 0.005s  \n",
      "\n",
      "2025-12-28 22:37:35 (213 MB/s) - ‘data/input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory for data if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# Download the dataset\n",
    "!wget -O data/input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3bbfe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data loaded. Length: 1115394 characters\n",
      "Tokenizer fitted. Vocabulary size: 1066\n",
      "Encoded data length: 416705 tokens\n",
      "First 10 encoded tokens: [536, 988, 77, 794, 465, 348, 416, 42, 837, 529]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.merges = []\n",
    "        self.vocab = {\"<UNK>\": 0}\n",
    "        self.id_to_token = {0: \"<UNK>\"}\n",
    "        self.next_id = 1\n",
    "\n",
    "    def _get_pairs(self, tokens):\n",
    "        \"\"\"\n",
    "        Get all adjacent pairs and their counts.\n",
    "\n",
    "        Args:\n",
    "            tokens: list of tokens\n",
    "\n",
    "        Returns:\n",
    "            Counter of pairs\n",
    "        \"\"\"\n",
    "        pairs = Counter()\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pairs[(tokens[i], tokens[i+1])] += 1\n",
    "        return pairs\n",
    "\n",
    "\n",
    "    def _merge_pair(self, tokens, pair, new_token):\n",
    "        \"\"\"\n",
    "        Merge all occurrences of pair into new_token.\n",
    "\n",
    "        Args:\n",
    "            tokens: list of tokens\n",
    "            pair: tuple (token1, token2) to merge\n",
    "            new_token: the merged token\n",
    "\n",
    "        Returns:\n",
    "            new list of tokens with merges applied\n",
    "        \"\"\"\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i<len(tokens):\n",
    "            if i+1<len(tokens) and (tokens[i], tokens[i+1]) == pair:\n",
    "                new_tokens.append(new_token)\n",
    "                i+=2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i+=1\n",
    "        return new_tokens\n",
    "\n",
    "\n",
    "    def fit(self, text, num_merges=10):\n",
    "        \"\"\"\n",
    "        Learn BPE merges from text.\n",
    "\n",
    "        Args:\n",
    "            text: training text\n",
    "            num_merges: number of merge operations to perform\n",
    "        \"\"\"\n",
    "        self.merges = []\n",
    "        self.vocab = {\"<UNK>\": 0}\n",
    "        self.id_to_token = {0: \"<UNK>\"}\n",
    "        self.next_id = 1\n",
    "        initial_chars = sorted(list(set(text)))\n",
    "        for char in initial_chars:\n",
    "            if char not in self.vocab:\n",
    "                self.vocab[char] = self.next_id\n",
    "                self.id_to_token[self.next_id] = char\n",
    "                self.next_id += 1\n",
    "\n",
    "        current_tokens = list(text)\n",
    "\n",
    "        for _ in range(num_merges):\n",
    "            pairs = self._get_pairs(current_tokens)\n",
    "            if not pairs:\n",
    "                break\n",
    "            most_frequent_pair = max(pairs, key=pairs.get)\n",
    "            new_token_str = \"\".join(most_frequent_pair)\n",
    "            if new_token_str not in self.vocab:\n",
    "                self.vocab[new_token_str] = self.next_id\n",
    "                self.id_to_token[self.next_id] = new_token_str\n",
    "                self.next_id += 1\n",
    "                self.merges.append((most_frequent_pair, new_token_str))\n",
    "            else:\n",
    "                if (most_frequent_pair, new_token_str) not in self.merges:\n",
    "                    self.merges.append((most_frequent_pair, new_token_str))\n",
    "\n",
    "            current_tokens = self._merge_pair(current_tokens, most_frequent_pair, new_token_str)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text using learned BPE merges.\n",
    "\n",
    "        Args:\n",
    "            text: string to encode\n",
    "\n",
    "        Returns:\n",
    "            list of token IDs\n",
    "        \"\"\"\n",
    "        initial_tokens_list = []\n",
    "        for char in text:\n",
    "            if char not in self.vocab:\n",
    "                self.vocab[char] = self.next_id\n",
    "                self.id_to_token[self.next_id] = char\n",
    "                self.next_id += 1\n",
    "            initial_tokens_list.append(char)\n",
    "        tokens_to_merge = list(initial_tokens_list)\n",
    "        for pair, new_token_str in self.merges:\n",
    "            tokens_to_merge = self._merge_pair(tokens_to_merge, pair, new_token_str)\n",
    "        encoded_ids = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens_to_merge]\n",
    "        return encoded_ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Convert token IDs back to text.\n",
    "\n",
    "        Args:\n",
    "            ids: list of token IDs\n",
    "\n",
    "        Returns:\n",
    "            decoded string\n",
    "        \"\"\"\n",
    "        decoded_tokens = [self.id_to_token.get(id, \"<UNK>\") for id in ids]\n",
    "        return \"\".join(decoded_tokens)\n",
    "\n",
    "\n",
    "with open('data/input.txt', 'r') as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.fit(text_data, num_merges=1000)\n",
    "\n",
    "\n",
    "encoded_data = tokenizer.encode(text_data)\n",
    "\n",
    "print(f\"Text data loaded. Length: {len(text_data)} characters\")\n",
    "print(f\"Tokenizer fitted. Vocabulary size: {len(tokenizer.vocab)}\")\n",
    "print(f\"Encoded data length: {len(encoded_data)} tokens\")\n",
    "print(f\"First 10 encoded tokens: {encoded_data[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d93712f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 375034\n",
      "Validation data length: 41671\n",
      "Training dataset sequences: 374778\n",
      "Validation dataset sequences: 41415\n",
      "Number of training batches: 11712\n",
      "Number of validation batches: 1295\n",
      "\n",
      "Sample input batch shape: torch.Size([32, 256])\n",
      "Sample target batch shape: torch.Size([32, 256])\n",
      "Sample inputs[0][:10]: [94, 635, 45, 389, 859, 70, 42, 131, 76, 77]\n",
      "Sample targets[0][:10]: [635, 45, 389, 859, 70, 42, 131, 76, 77, 844]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "max_seq_len = 256\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "data = torch.tensor(encoded_data, dtype=torch.long)\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(data))\n",
    "\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, max_seq_len):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.data) < self.max_seq_len + 1:\n",
    "            return 0\n",
    "        return len(self.data) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx : idx + self.max_seq_len + 1]\n",
    "        inputs = chunk[:-1]\n",
    "        targets = chunk[1:]\n",
    "        return inputs, targets\n",
    "train_dataset = TextDataset(train_data, max_seq_len)\n",
    "val_dataset = TextDataset(val_data, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training data length: {len(train_data)}\")\n",
    "print(f\"Validation data length: {len(val_data)}\")\n",
    "print(f\"Training dataset sequences: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset sequences: {len(val_dataset)}\")\n",
    "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
    "print(f\"Number of validation batches: {len(val_dataloader)}\")\n",
    "\n",
    "# Example of one batch\n",
    "if len(train_dataloader) > 0:\n",
    "    sample_inputs, sample_targets = next(iter(train_dataloader))\n",
    "    print(f\"\\nSample input batch shape: {sample_inputs.shape}\")\n",
    "    print(f\"Sample target batch shape: {sample_targets.shape}\")\n",
    "    print(f\"Sample inputs[0][:10]: {sample_inputs[0][:10].tolist()}\")\n",
    "    print(f\"Sample targets[0][:10]: {sample_targets[0][:10].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9356f033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model initialized with vocab_size=1066, d_model=256, num_heads=8, num_layers=6\n",
      "Model is on device: cuda:0\n",
      "Optimizer (AdamW) initialized.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "model = DecoderLM(vocab_size, d_model, num_heads, num_layers, max_seq_len=max_seq_len)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), weight_decay=0.01)\n",
    "\n",
    "print(f\"Model initialized with vocab_size={vocab_size}, d_model={d_model}, num_heads={num_heads}, num_layers={num_layers}\")\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "print(\"Optimizer (AdamW) initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992848f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, past_kv=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_q = query.size(1)\n",
    "        seq_len_k_new = key.size(1)\n",
    "\n",
    "        Q_proj = self.W_Q(query)\n",
    "        K_proj = self.W_K(key)\n",
    "        V_proj = self.W_V(value)\n",
    "\n",
    "        Q = Q_proj.view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K_proj.view(batch_size, seq_len_k_new, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V_proj.view(batch_size, seq_len_k_new, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        if past_kv is not None:\n",
    "            K_past, V_past = past_kv\n",
    "            K = torch.cat([K_past, K], dim=2)\n",
    "            V = torch.cat([V_past, V], dim=2)\n",
    "        present_kv = (K, V)\n",
    "\n",
    "        seq_len_k_full = K.size(2)\n",
    "\n",
    "        scores = (Q @ K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores + mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        output_attn = weights @ V\n",
    "\n",
    "        output_attn = output_attn.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        output = self.W_O(output_attn)\n",
    "\n",
    "        return output, weights, present_kv\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    def forward(self, x, past_kv=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        if past_kv is not None:\n",
    "            mask = None\n",
    "        else:\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "            mask = mask.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        output, attn_weights, present_kv = self.mha(x, x, x, mask=mask, past_kv=past_kv)\n",
    "        return output, attn_weights, present_kv\n",
    "\n",
    "\n",
    "class MyLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        var = torch.var(x, dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=None):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.ln1 = MyLayerNorm(d_model)\n",
    "        self.ln2 = MyLayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, past_kv=None):\n",
    "        attn_output, _, present_kv = self.attn(self.ln1(x), past_kv=past_kv)\n",
    "        x = x + attn_output\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x, present_kv\n",
    "\n",
    "\n",
    "class DecoderLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, num_heads) for _ in range(num_layers)])\n",
    "\n",
    "        self.ln_f = MyLayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, past_kv_list=None):\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        tok_emb = self.token_embed(x)\n",
    "\n",
    "        if past_kv_list is not None:\n",
    "            past_seq_len = past_kv_list[0][0].shape[2] if past_kv_list else 0\n",
    "            current_position_idx = past_seq_len\n",
    "            positions_to_embed = torch.tensor([current_position_idx], device=x.device)\n",
    "            pos_emb = self.pos_embed(positions_to_embed)\n",
    "            h = tok_emb + pos_emb.unsqueeze(0)\n",
    "        else:\n",
    "            positions = torch.arange(seq_len, device=x.device)\n",
    "            pos_emb = self.pos_embed(positions)\n",
    "            h = tok_emb + pos_emb.unsqueeze(0)\n",
    "\n",
    "        present_kv_list = []\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            past_kv_i = past_kv_list[i] if past_kv_list else None\n",
    "            h, present_kv_i = block(h, past_kv=past_kv_i)\n",
    "            present_kv_list.append(present_kv_i)\n",
    "\n",
    "        h = self.ln_f(h)\n",
    "        logits = self.head(h)\n",
    "\n",
    "        return logits, present_kv_list\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "\n",
    "model = DecoderLM(vocab_size, d_model, num_heads, num_layers, max_seq_len=max_seq_len)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), weight_decay=0.01)\n",
    "\n",
    "print(f\"Model initialized with vocab_size={vocab_size}, d_model={d_model}, num_heads={num_heads}, num_layers={num_layers}\")\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "print(\"Optimizer (AdamW) initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7deb9435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1/10, Train Loss: 1.3008, LR: 0.000300\n",
      "Epoch 2/10, Train Loss: 0.0168, LR: 0.000291\n",
      "Validation Loss: 0.0271, Perplexity: 1.03\n",
      "Saved new best model with validation loss: 0.0271\n",
      "Epoch 3/10, Train Loss: 0.0149, LR: 0.000266\n",
      "Epoch 4/10, Train Loss: 0.0137, LR: 0.000228\n",
      "Validation Loss: 0.0268, Perplexity: 1.03\n",
      "Saved new best model with validation loss: 0.0268\n",
      "Epoch 5/10, Train Loss: 0.0125, LR: 0.000180\n",
      "Epoch 6/10, Train Loss: 0.0109, LR: 0.000130\n",
      "Validation Loss: 0.0278, Perplexity: 1.03\n",
      "Epoch 7/10, Train Loss: 0.0088, LR: 0.000083\n",
      "Epoch 8/10, Train Loss: 0.0065, LR: 0.000044\n",
      "Validation Loss: 0.0311, Perplexity: 1.03\n",
      "Epoch 9/10, Train Loss: 0.0046, LR: 0.000019\n",
      "Epoch 10/10, Train Loss: 0.0035, LR: 0.000010\n",
      "Validation Loss: 0.0336, Perplexity: 1.03\n",
      "Training finished.\n",
      "Training metrics collected and stored.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_lr(step, warmup_steps, total_steps, max_lr, min_lr):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step / warmup_steps)\n",
    "    elif step > total_steps:\n",
    "        return min_lr\n",
    "    else:\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "def set_optimizer_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "            logits, _ = model(batch_inputs)\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                batch_targets.view(-1)\n",
    "            )\n",
    "            total_loss += loss.item() * batch_targets.numel()\n",
    "            total_tokens += batch_targets.numel()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    model.train()\n",
    "    return avg_loss\n",
    "\n",
    "def compute_perplexity(avg_loss):\n",
    "    return math.exp(avg_loss)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "total_training_steps = num_epochs * len(train_dataloader)\n",
    "warmup_steps = int(0.1 * total_training_steps)\n",
    "max_lr = 3e-4\n",
    "min_lr = 1e-5\n",
    "\n",
    "eval_interval = 2\n",
    "save_interval = 2\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "perplexities = []\n",
    "lrs = []\n",
    "\n",
    "print(\"Starting training\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    for batch_inputs, batch_targets in train_dataloader:\n",
    "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "        lr = get_lr(global_step, warmup_steps, total_training_steps, max_lr, min_lr)\n",
    "        set_optimizer_lr(optimizer, lr)\n",
    "        lrs.append(lr)\n",
    "\n",
    "        logits, _ = model(batch_inputs)\n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            batch_targets.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_losses.append(loss.item())\n",
    "        global_step += 1\n",
    "\n",
    "    avg_epoch_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(avg_epoch_train_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_epoch_train_loss:.4f}, LR: {lrs[-1]:.6f}\")\n",
    "\n",
    "    if (epoch + 1) % eval_interval == 0:\n",
    "        val_loss = evaluate(model, val_dataloader, criterion, device)\n",
    "        perplexity = compute_perplexity(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        perplexities.append(perplexity)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_checkpoint.pt')\n",
    "            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "metrics = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'perplexities': perplexities,\n",
    "    'lrs': lrs,\n",
    "    'best_val_loss': best_val_loss\n",
    "}\n",
    "\n",
    "print(\"Training metrics collected and stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6190d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DecoderLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model checkpoint...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Recreate the model with the same architecture\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model_test = \u001b[43mDecoderLM\u001b[49m(vocab_size, d_model, num_heads, num_layers, max_seq_len=max_seq_len)\n\u001b[32m      6\u001b[39m model_test.to(device)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load the checkpoint\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'DecoderLM' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the trained model checkpoint\n",
    "print(\"Loading model checkpoint...\")\n",
    "\n",
    "# Recreate the model with the same architecture\n",
    "model_test = DecoderLM(vocab_size, d_model, num_heads, num_layers, max_seq_len=max_seq_len)\n",
    "model_test.to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('best_model_checkpoint.pt', map_location=device)\n",
    "model_test.load_state_dict(checkpoint)\n",
    "model_test.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model is on device: {next(model_test.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=200, temperature=1.0, top_k=None, top_p=None, use_kv_cache=True):\n",
    "    \"\"\"\n",
    "    Generate text using the model with KV caching support.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained DecoderLM model\n",
    "        tokenizer: The BPETokenizer\n",
    "        prompt: Starting text prompt\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: If set, only sample from top k tokens\n",
    "        top_p: If set, use nucleus sampling with this probability\n",
    "        use_kv_cache: Whether to use KV caching for faster generation\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    generated_ids = input_ids.copy()\n",
    "    past_kv_list = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # First pass: process the entire prompt\n",
    "        if use_kv_cache:\n",
    "            logits, past_kv_list = model(input_tensor, past_kv_list=None)\n",
    "            # Only use the last token's logits for next token prediction\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "        else:\n",
    "            # Without KV cache, we'll process the full sequence each time (slower)\n",
    "            logits, _ = model(input_tensor, past_kv_list=None)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "        \n",
    "        # Generate tokens one by one\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Apply top-k filtering if specified\n",
    "            if top_k is not None:\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "                # Create a new tensor with -inf for non-top-k tokens\n",
    "                filtered_logits = torch.full_like(next_token_logits, float('-inf'))\n",
    "                filtered_logits[top_k_indices] = top_k_logits\n",
    "                next_token_logits = filtered_logits\n",
    "            \n",
    "            # Apply top-p (nucleus) sampling if specified\n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                # Remove tokens with cumulative probability above the threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                next_token_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            generated_ids.append(next_token_id)\n",
    "            \n",
    "            # Prepare next input (single token for KV caching)\n",
    "            if use_kv_cache:\n",
    "                next_input = torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
    "                logits, past_kv_list = model(next_input, past_kv_list=past_kv_list)\n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "            else:\n",
    "                # Without cache, process the full sequence again\n",
    "                input_tensor = torch.tensor([generated_ids], dtype=torch.long).to(device)\n",
    "                logits, _ = model(input_tensor, past_kv_list=None)\n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(generated_ids)\n",
    "    return generated_text\n",
    "\n",
    "print(\"Text generation function created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d61382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with various prompts\n",
    "print(\"=\" * 80)\n",
    "print(\"Testing Model Text Generation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test prompts inspired by Shakespeare\n",
    "test_prompts = [\n",
    "    \"To be or not to be\",\n",
    "    \"Once upon a time\",\n",
    "    \"The king said\",\n",
    "    \"Romeo and Juliet\",\n",
    "    \"All the world's a stage\"\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating text with different prompts...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}: Prompt = '{prompt}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        generated = generate_text(\n",
    "            model_test, \n",
    "            tokenizer, \n",
    "            prompt, \n",
    "            max_new_tokens=150,\n",
    "            temperature=0.8,\n",
    "            use_kv_cache=True\n",
    "        )\n",
    "        print(f\"\\nGenerated text:\\n{generated}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a192249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different temperature settings\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Testing Different Temperature Settings\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "prompt = \"To be or not to be\"\n",
    "temperatures = [0.3, 0.7, 1.0, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    try:\n",
    "        generated = generate_text(\n",
    "            model_test,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            max_new_tokens=100,\n",
    "            temperature=temp,\n",
    "            use_kv_cache=True\n",
    "        )\n",
    "        print(f\"\\nGenerated text:\\n{generated}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02856888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test KV caching speedup\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Testing KV Caching Performance\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "prompt = \"The king said to his court\"\n",
    "num_tokens = 200\n",
    "\n",
    "# Test with KV cache\n",
    "start_time = time.time()\n",
    "generated_with_cache = generate_text(\n",
    "    model_test,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=num_tokens,\n",
    "    temperature=0.8,\n",
    "    use_kv_cache=True\n",
    ")\n",
    "time_with_cache = time.time() - start_time\n",
    "\n",
    "# Test without KV cache\n",
    "start_time = time.time()\n",
    "generated_without_cache = generate_text(\n",
    "    model_test,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=num_tokens,\n",
    "    temperature=0.8,\n",
    "    use_kv_cache=False\n",
    ")\n",
    "time_without_cache = time.time() - start_time\n",
    "\n",
    "speedup = time_without_cache / time_with_cache\n",
    "\n",
    "print(f\"\\nGeneration time with KV cache: {time_with_cache:.4f} seconds\")\n",
    "print(f\"Generation time without KV cache: {time_without_cache:.4f} seconds\")\n",
    "print(f\"Speedup: {speedup:.2f}x\")\n",
    "print(f\"\\nGenerated text (with cache):\\n{generated_with_cache[:200]}...\")\n",
    "print(f\"\\nGenerated text (without cache):\\n{generated_without_cache[:200]}...\")\n",
    "print(f\"\\nOutputs match: {generated_with_cache == generated_without_cache}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
